{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Earnings Surprises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict the magnitude of company's upcoming earnings announcement using a machine learning classification model. The model is trained on three types of data: earnings, pricing, and technical price action data. The optimized model outputs a result into one of three classes: positive, neutral, or negative. A 'positive' classification indicates a predicted surprise >15% of the estimated eps, a 'negative' classification indicates a predicted surprise <-15% of the estimated eps, and a 'neutral' classification indicates no predicted surprise (15% < x < -15%). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for training and testing the model came from several external data providers. Earning and pricing data was collected from Financial Modeling Prep's historical earnings calendar and daily indicator endpoints. Technical data is collected from FMP Cloud's daily technical indicator endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema below outlines the database architecture into an AWS RDS MySQL database:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Untitled Workspace (1)](https://user-images.githubusercontent.com/45079557/150410944-eb8c8e30-ac2d-4f23-bb03-cb5c3f489cfb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pymysql\n",
    "import seaborn as sns\n",
    "from decouple import config\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for AWS database\n",
    "aws_hostname = config(\"AWS_HOST\")\n",
    "aws_database = config(\"AWS_DB\")\n",
    "aws_username = config(\"AWS_USER\")\n",
    "aws_password = config(\"AWS_PASS\")\n",
    "aws_port = config(\"AWS_PORT\")\n",
    "\n",
    "# Pull API keys from .env file\n",
    "FMP_API_KEY = config(\"FMP_API_KEY\")\n",
    "FMP_CLOUD_API_KEY = config(\"FMP_CLOUD_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pymysql.connect(host=aws_hostname,user=aws_username, password=aws_password, database='rds-python', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)\n",
    "cursor = db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Data from MySQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "SELECT *, \n",
    "LAG(perc_change) OVER(PARTITION BY symbol ORDER BY STR_TO_DATE(`date`, '%c/%e/%y')) AS lastSurp, \n",
    "LAG(perc_change, 2) OVER(PARTITION BY symbol ORDER BY STR_TO_DATE(`date`, '%c/%e/%y')) AS last2Surp,\n",
    "\n",
    "LAG(eps) OVER(PARTITION BY symbol ORDER BY STR_TO_DATE(`date`, '%c/%e/%y')) AS lastEps,\n",
    "LAG(eps, 2) OVER(PARTITION BY symbol ORDER BY STR_TO_DATE(`date`, '%c/%e/%y')) AS last2Eps,\n",
    "\n",
    "LAG(epsEstimated) OVER(PARTITION BY symbol ORDER BY STR_TO_DATE(`date`, '%c/%e/%y')) AS lastEst,\n",
    "LAG(epsEstimated, 2) OVER(PARTITION BY symbol ORDER BY STR_TO_DATE(`date`, '%c/%e/%y')) AS last2Est\n",
    "FROM (\n",
    "    SELECT *, COALESCE((eps - epsEstimated) / ABS(epsEstimated) * 100,0) AS perc_change\n",
    "    FROM train_agg\n",
    "    ORDER BY STR_TO_DATE(`date`, '%c/%e/%y')\n",
    ")x\n",
    "\"\"\")\n",
    "train = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"...Start...\")\n",
    "print(train_df.head(2))\n",
    "print(\"...End...\")\n",
    "print(train_df.tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_NaN = train_df.isnull()\n",
    "row_has_NaN = is_NaN.any(axis=1)\n",
    "forecast_these = train_df[row_has_NaN]\n",
    "print(len(forecast_these))\n",
    "print(forecast_these.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df[\"eps\"].notna()]\n",
    "df = train_df\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA/Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pd.to_datetime(df[\"date\"]), df[\"perc_change\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few outliers are present in the percentage difference between EPS and EPS estimated. Therefore, we will fitler out rows that are greater than 3 or less than -3 standard deviations away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.perc_change.between(df.perc_change.quantile(.01), df.perc_change.quantile(.99))]\n",
    "print(\"Earnings Surprise Average: {}\".format(df[\"perc_change\"].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Historical Earnings Surprises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 100\n",
    "plt.hist(df[\"perc_change\"], num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('EPS Difference (EPS - EPS Estimated)')\n",
    "plt.ylabel('Number of Earnings')\n",
    "plt.title('Histogram of Earnings Surprises')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('visuals/histogram_eps_diff.png', facecolor='white', transparent=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing outliers, we can see that the majority of historical earnings follow a normal distribution around the mean of 9%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Significant Earnings Surprise Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_surp_thres = 15\n",
    "neg_surp_thres = -15\n",
    "\n",
    "pos_surp = df[(df.perc_change > pos_surp_thres)]\n",
    "neg_surp = df[(df.perc_change < neg_surp_thres)]\n",
    "neu_surp = df[(df.perc_change < pos_surp_thres) & (df.perc_change > neg_surp_thres)]\n",
    "\n",
    "x = [\"Positive\", \"Neutral\", \"Negative\"]\n",
    "surprises = [len(pos_surp), len(neu_surp), len(neg_surp)]\n",
    "\n",
    "print(len(pos_surp))\n",
    "print(\"-------------\")\n",
    "print(len(neg_surp))\n",
    "print(\"-------------\")\n",
    "print(len(neu_surp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breakdown of the total number of each type of surprise in the dataset\n",
    "# Positive: >15% surprise\n",
    "# Neutral: <15% surprise and >-15% surprise\n",
    "# Negative: <-15% surprise\n",
    "plt.bar(x, surprises, color=['green', 'yellow', 'red'], alpha=0.5)\n",
    "plt.ylabel('Number of Earnings')\n",
    "plt.title('Earnings Surprise Breakdown')\n",
    "plt.tight_layout()\n",
    "plt.savefig('visuals/earn_bar.png', facecolor='white', transparent=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the bar chart above, there are significantly more positive earnings surprises than negative earnings surprises. Therefore, it might be more lucrative, you only go long plays on earnings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Earnings Surprise Breakdown Based on Earnings Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"time\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_bmo = df[(df.perc_change > pos_surp_thres) & (df[\"time\"] == 'bmo')]\n",
    "pos_amc = df[(df.perc_change > pos_surp_thres) & (df[\"time\"] == 'amc')]\n",
    "neu_bmo = df[(df.perc_change < pos_surp_thres) & (df.perc_change > neg_surp_thres) & (df[\"time\"] == 'bmo')]\n",
    "neu_amc = df[(df.perc_change < pos_surp_thres) & (df.perc_change > neg_surp_thres) & (df[\"time\"] == 'amc')]\n",
    "neg_bmo = df[(df.perc_change < neg_surp_thres) & (df[\"time\"] == 'bmo')]\n",
    "neg_amc = df[(df.perc_change < neg_surp_thres) & (df[\"time\"] == 'amc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\"Positive\", \"Neutral\", \"Negative\"]\n",
    "surprises_bmo = [len(pos_bmo), len(neu_bmo), len(neg_bmo)]\n",
    "surprises_amc = [len(pos_amc), len(neu_amc), len(neg_amc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.arange(3) \n",
    "width = 0.35       \n",
    "plt.bar(ind, surprises_bmo, width, label='Before Market Open', color='blue', alpha=0.5)\n",
    "plt.bar(ind+width, surprises_amc, width,\n",
    "    label='After Market Close', color='red', alpha=0.5)\n",
    "\n",
    "plt.ylabel('Number of Earnings')\n",
    "plt.title('Earnings Surprise Breakdown By Earnings Time')\n",
    "\n",
    "plt.xticks(ind + width / 2, ('Positive', 'Neutral', 'Negative'))\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.savefig('visuals/earn_bar_time.png', facecolor='white', transparent=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no significant difference between earnings surprise and when the earnings is announced (before market open or after market close)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Earnings Surprise Breakdown by Day of Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to find the date information using earnings date column\n",
    "dates = pd.to_datetime(df[\"date\"])\n",
    "df[\"dow\"] = dates.dt.dayofweek\n",
    "print(df[\"dow\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_m = df[(df.perc_change > pos_surp_thres) & (df[\"dow\"] == 0)]\n",
    "pos_tu = df[(df.perc_change > pos_surp_thres) & (df[\"dow\"] == 1)]\n",
    "pos_w = df[(df.perc_change > pos_surp_thres) & (df[\"dow\"] == 2)]\n",
    "pos_th = df[(df.perc_change > pos_surp_thres) & (df[\"dow\"] == 3)]\n",
    "pos_f = df[(df.perc_change > pos_surp_thres) & (df[\"dow\"] == 4)]\n",
    "\n",
    "neu_m = df[(df.perc_change < pos_surp_thres) & (df.perc_change > neg_surp_thres) & (df[\"dow\"] == 0)]\n",
    "neu_tu = df[(df.perc_change < pos_surp_thres) & (df.perc_change > neg_surp_thres) & (df[\"dow\"] == 1)]\n",
    "neu_w = df[(df.perc_change < pos_surp_thres) & (df.perc_change > neg_surp_thres) & (df[\"dow\"] == 2)]\n",
    "neu_th = df[(df.perc_change < pos_surp_thres) & (df.perc_change > neg_surp_thres) & (df[\"dow\"] == 3)]\n",
    "neu_f = df[(df.perc_change < pos_surp_thres) & (df.perc_change > neg_surp_thres) & (df[\"dow\"] == 4)]\n",
    "\n",
    "neg_m = df[(df.perc_change < neg_surp_thres) & (df[\"dow\"] == 0)]\n",
    "neg_tu = df[(df.perc_change < neg_surp_thres) & (df[\"dow\"] == 1)]\n",
    "neg_w = df[(df.perc_change < neg_surp_thres) & (df[\"dow\"] == 2)]\n",
    "neg_th = df[(df.perc_change < neg_surp_thres) & (df[\"dow\"] == 3)]\n",
    "neg_f = df[(df.perc_change < neg_surp_thres) & (df[\"dow\"] == 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\"Positive\", \"Neutral\", \"Negative\"]\n",
    "surprises_m = [len(pos_m), len(neu_m), len(neg_m)]\n",
    "surprises_tu = [len(pos_tu), len(neu_tu), len(neg_tu)]\n",
    "surprises_w = [len(pos_w), len(neu_w), len(neg_w)]\n",
    "surprises_th = [len(pos_th), len(neu_th), len(neg_th)]\n",
    "surprises_f = [len(pos_f), len(neu_f), len(neg_f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.arange(3) \n",
    "width = 0.15\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(ind, surprises_m, width, label='Monday', alpha=0.5)\n",
    "plt.bar(ind+width, surprises_tu, width, label='Tuesday', alpha=0.5)\n",
    "plt.bar(ind+(2*width), surprises_w, width, label='Wednesday', alpha=0.5)\n",
    "plt.bar(ind+(3*width), surprises_th, width, label='Thursday', alpha=0.5)\n",
    "plt.bar(ind+(4*width), surprises_f, width, label='Friday', alpha=0.5)\n",
    "\n",
    "plt.ylabel('Number of Earnings')\n",
    "plt.title('Earnings Surprise Breakdown By Day of Week')\n",
    "\n",
    "plt.xticks(ind + width*2, ('Positive', 'Neutral', 'Negative'))\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.savefig('visuals/earn_bar_dow.png', facecolor='white', transparent=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create lagging features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added lagging features derived from earnings data within the SQL statement. We already have lagging features for pricing data with the technical indicators. Now we can hide symbol as feature from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values within one lag\n",
    "numer = df[\"lastEps\"].isnull().sum()\n",
    "denom = len(df[\"perc_change\"])\n",
    "\n",
    "null_perc = numer/denom * 100\n",
    "print(null_perc)\n",
    "print(numer)\n",
    "print(denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values within two lags\n",
    "numer = df[\"last2Eps\"].isnull().sum()\n",
    "denom = len(df[\"perc_change\"])\n",
    "\n",
    "null_perc = numer/denom * 100\n",
    "print(null_perc)\n",
    "print(numer)\n",
    "print(denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop symbol\n",
    "df = df.drop([\"symbol\"], axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"month\"] = pd.DatetimeIndex(df[\"date\"]).month\n",
    "print(df[\"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"day\"] = pd.DatetimeIndex(df[\"date\"]).day\n",
    "print(df[\"day\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"year\"] = pd.DatetimeIndex(df[\"date\"]).year\n",
    "print(df[\"year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop date\n",
    "df = df.drop([\"date\"], axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode time variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time\"].loc[(df['time'] == \"bmo\")] = 0\n",
    "df[\"time\"].loc[(df['time'] == \"amc\")] = 1\n",
    "df[\"time\"] = df[\"time\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop id and percentage change\n",
    "# Removing percentage change because it wont be known at time of prediction\n",
    "df = df.drop([\"id\", \"perc_change\"], axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset\n",
    "X = df.iloc[:, 1:]\n",
    "y = df.iloc[:, :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "regressor.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d032f0ee728082bb6a4d2cd2973e452a9a829bb860acc9dec402cf46b334b4c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
